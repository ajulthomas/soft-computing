{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# add more col name to columns\n",
    "columns.insert(0, 'time')\n",
    "columns.append('amount')\n",
    "columns.append('target')\n",
    "columns\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('./data/card.csv', names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>amount</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  amount  target  \n",
       "0 -0.189115  0.133558 -0.021053  149.62       0  \n",
       "1  0.125895 -0.008983  0.014724    2.69       0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66       0  \n",
       "3 -0.221929  0.062723  0.061458  123.50       0  \n",
       "4  0.502292  0.219422  0.215153   69.99       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictor and traget variables\n",
    "\n",
    "X = df.loc[:, 'time':'amount']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284807, 30), (284807,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape pf the data\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data into train_validate and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(1/8), random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199364, 30), (28481, 30), (56962, 30), (199364,), (28481,), (56962,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyper_param_options = [\n",
    "    # activation function relu\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # # activation function tanh\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # change the acrhitecture\n",
    "    # {'hidden_layer_sizes': (60, 30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20, 10), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20, 10), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True}, \n",
    "    # {'hidden_layer_sizes': (32, 18, 6, 2), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True}\n",
    "\n",
    "    # change initial learning rate with activation function tanh, batch size 50 and architecture 32\n",
    "    # {'hidden_layer_sizes': (20), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (28), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (32), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (36), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "\n",
    "    # best params so far with activation function tanh, batch size 100, learning_rate_init 0.0001 and architecture 32\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 75, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 150, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "\n",
    "    # best params so far with activation function tanh, batch size 100, learning_rate_init 0.001, solver sgd, learning_rate adaptive and architecture 30\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def train_model(id, hyper_params):\n",
    "    mlp_classifier = MLPClassifier(**hyper_params).fit(X_train_scaled, y_train)\n",
    "    train_score = mlp_classifier.score(X_train_scaled, y_train)\n",
    "    val_score = mlp_classifier.score(X_val_scaled, y_val)\n",
    "    test_score = mlp_classifier.score(X_test_scaled, y_test)\n",
    "    return {\n",
    "        'id': id,\n",
    "        'hidden_layers': mlp_classifier.n_layers_ - 2,\n",
    "        'architecture': mlp_classifier.hidden_layer_sizes,\n",
    "        'activation': hyper_params['activation'],\n",
    "        'solver': hyper_params['solver'],\n",
    "        'batch_size': hyper_params['batch_size'],\n",
    "        'learning_rate_init': hyper_params['learning_rate_init'],\n",
    "        'learning_rate': hyper_params['learning_rate'],\n",
    "        'iterations': mlp_classifier.n_iter_,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.01339956\n",
      "Iteration 2, loss = 0.00339689\n",
      "Iteration 3, loss = 0.00308715\n",
      "Iteration 4, loss = 0.00292665\n",
      "Iteration 5, loss = 0.00278040\n",
      "Iteration 6, loss = 0.00269872\n",
      "Iteration 7, loss = 0.00263662\n",
      "Iteration 8, loss = 0.00258221\n",
      "Iteration 9, loss = 0.00254363\n",
      "Iteration 10, loss = 0.00250517\n",
      "Iteration 11, loss = 0.00247824\n",
      "Iteration 12, loss = 0.00243345\n",
      "Iteration 13, loss = 0.00241175\n",
      "Iteration 14, loss = 0.00238691\n",
      "Iteration 15, loss = 0.00237011\n",
      "Iteration 16, loss = 0.00234250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 17, loss = 0.00229375\n",
      "Iteration 18, loss = 0.00228691\n",
      "Iteration 19, loss = 0.00228229\n",
      "Iteration 20, loss = 0.00227835\n",
      "Iteration 21, loss = 0.00227499\n",
      "Iteration 22, loss = 0.00227073\n",
      "Iteration 23, loss = 0.00226701\n",
      "Iteration 24, loss = 0.00226398\n",
      "Iteration 25, loss = 0.00226003\n",
      "Iteration 26, loss = 0.00225535\n",
      "Iteration 27, loss = 0.00225369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 28, loss = 0.00224118\n",
      "Iteration 29, loss = 0.00224044\n",
      "Iteration 30, loss = 0.00223980\n",
      "Iteration 31, loss = 0.00223897\n",
      "Iteration 32, loss = 0.00223830\n",
      "Iteration 33, loss = 0.00223760\n",
      "Iteration 34, loss = 0.00223690\n",
      "Iteration 35, loss = 0.00223632\n",
      "Iteration 36, loss = 0.00223548\n",
      "Iteration 37, loss = 0.00223491\n",
      "Iteration 38, loss = 0.00223434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 39, loss = 0.00223172\n",
      "Iteration 40, loss = 0.00223159\n",
      "Iteration 41, loss = 0.00223145\n",
      "Iteration 42, loss = 0.00223130\n",
      "Iteration 43, loss = 0.00223116\n",
      "Iteration 44, loss = 0.00223103\n",
      "Iteration 45, loss = 0.00223091\n",
      "Iteration 46, loss = 0.00223076\n",
      "Iteration 47, loss = 0.00223063\n",
      "Iteration 48, loss = 0.00223049\n",
      "Iteration 49, loss = 0.00223035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 50, loss = 0.00222984\n",
      "Iteration 51, loss = 0.00222982\n",
      "Iteration 52, loss = 0.00222979\n",
      "Iteration 53, loss = 0.00222977\n",
      "Iteration 54, loss = 0.00222973\n",
      "Iteration 55, loss = 0.00222971\n",
      "Iteration 56, loss = 0.00222969\n",
      "Iteration 57, loss = 0.00222965\n",
      "Iteration 58, loss = 0.00222963\n",
      "Iteration 59, loss = 0.00222960\n",
      "Iteration 60, loss = 0.00222957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 61, loss = 0.00222947\n",
      "Iteration 62, loss = 0.00222946\n",
      "Iteration 63, loss = 0.00222946\n",
      "Iteration 64, loss = 0.00222945\n",
      "Iteration 65, loss = 0.00222945\n",
      "Iteration 66, loss = 0.00222944\n",
      "Iteration 67, loss = 0.00222944\n",
      "Iteration 68, loss = 0.00222943\n",
      "Iteration 69, loss = 0.00222943\n",
      "Iteration 70, loss = 0.00222942\n",
      "Iteration 71, loss = 0.00222942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 72, loss = 0.00222939\n",
      "Iteration 73, loss = 0.00222939\n",
      "Iteration 74, loss = 0.00222939\n",
      "Iteration 75, loss = 0.00222939\n",
      "Iteration 76, loss = 0.00222939\n",
      "Iteration 77, loss = 0.00222939\n",
      "Iteration 78, loss = 0.00222939\n",
      "Iteration 79, loss = 0.00222939\n",
      "Iteration 80, loss = 0.00222939\n",
      "Iteration 81, loss = 0.00222938\n",
      "Iteration 82, loss = 0.00222938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 0, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           0\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init        0.01\n",
      "learning_rate         adaptive\n",
      "iterations                  82\n",
      "train_score           0.999594\n",
      "val_score             0.999438\n",
      "test_score            0.999508\n",
      "Name: 0, dtype: object\n",
      "Training model 1, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.07498791\n",
      "Iteration 2, loss = 0.01256333\n",
      "Iteration 3, loss = 0.00776980\n",
      "Iteration 4, loss = 0.00604111\n",
      "Iteration 5, loss = 0.00516973\n",
      "Iteration 6, loss = 0.00465278\n",
      "Iteration 7, loss = 0.00431198\n",
      "Iteration 8, loss = 0.00407061\n",
      "Iteration 9, loss = 0.00389062\n",
      "Iteration 10, loss = 0.00374895\n",
      "Iteration 11, loss = 0.00363354\n",
      "Iteration 12, loss = 0.00353650\n",
      "Iteration 13, loss = 0.00345526\n",
      "Iteration 14, loss = 0.00338395\n",
      "Iteration 15, loss = 0.00332057\n",
      "Iteration 16, loss = 0.00326392\n",
      "Iteration 17, loss = 0.00321567\n",
      "Iteration 18, loss = 0.00317293\n",
      "Iteration 19, loss = 0.00313603\n",
      "Iteration 20, loss = 0.00310381\n",
      "Iteration 21, loss = 0.00307536\n",
      "Iteration 22, loss = 0.00304993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 0.00303032\n",
      "Iteration 24, loss = 0.00302570\n",
      "Iteration 25, loss = 0.00302123\n",
      "Iteration 26, loss = 0.00301681\n",
      "Iteration 27, loss = 0.00301256\n",
      "Iteration 28, loss = 0.00300829\n",
      "Iteration 29, loss = 0.00300411\n",
      "Iteration 30, loss = 0.00300000\n",
      "Iteration 31, loss = 0.00299597\n",
      "Iteration 32, loss = 0.00299209\n",
      "Iteration 33, loss = 0.00298815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 34, loss = 0.00298469\n",
      "Iteration 35, loss = 0.00298394\n",
      "Iteration 36, loss = 0.00298316\n",
      "Iteration 37, loss = 0.00298240\n",
      "Iteration 38, loss = 0.00298166\n",
      "Iteration 39, loss = 0.00298090\n",
      "Iteration 40, loss = 0.00298015\n",
      "Iteration 41, loss = 0.00297939\n",
      "Iteration 42, loss = 0.00297865\n",
      "Iteration 43, loss = 0.00297790\n",
      "Iteration 44, loss = 0.00297716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 45, loss = 0.00297648\n",
      "Iteration 46, loss = 0.00297633\n",
      "Iteration 47, loss = 0.00297618\n",
      "Iteration 48, loss = 0.00297604\n",
      "Iteration 49, loss = 0.00297589\n",
      "Iteration 50, loss = 0.00297574\n",
      "Iteration 51, loss = 0.00297559\n",
      "Iteration 52, loss = 0.00297545\n",
      "Iteration 53, loss = 0.00297530\n",
      "Iteration 54, loss = 0.00297515\n",
      "Iteration 55, loss = 0.00297500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 56, loss = 0.00297487\n",
      "Iteration 57, loss = 0.00297484\n",
      "Iteration 58, loss = 0.00297481\n",
      "Iteration 59, loss = 0.00297478\n",
      "Iteration 60, loss = 0.00297475\n",
      "Iteration 61, loss = 0.00297472\n",
      "Iteration 62, loss = 0.00297469\n",
      "Iteration 63, loss = 0.00297466\n",
      "Iteration 64, loss = 0.00297463\n",
      "Iteration 65, loss = 0.00297460\n",
      "Iteration 66, loss = 0.00297457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 67, loss = 0.00297455\n",
      "Iteration 68, loss = 0.00297454\n",
      "Iteration 69, loss = 0.00297453\n",
      "Iteration 70, loss = 0.00297453\n",
      "Iteration 71, loss = 0.00297452\n",
      "Iteration 72, loss = 0.00297452\n",
      "Iteration 73, loss = 0.00297451\n",
      "Iteration 74, loss = 0.00297450\n",
      "Iteration 75, loss = 0.00297450\n",
      "Iteration 76, loss = 0.00297449\n",
      "Iteration 77, loss = 0.00297449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 1, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           1\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init       0.001\n",
      "learning_rate         adaptive\n",
      "iterations                  77\n",
      "train_score           0.999473\n",
      "val_score             0.999368\n",
      "test_score             0.99935\n",
      "Name: 1, dtype: object\n",
      "Training model 2, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.29704887\n",
      "Iteration 2, loss = 0.13460608\n",
      "Iteration 3, loss = 0.08152229\n",
      "Iteration 4, loss = 0.05693445\n",
      "Iteration 5, loss = 0.04322947\n",
      "Iteration 6, loss = 0.03465433\n",
      "Iteration 7, loss = 0.02884975\n",
      "Iteration 8, loss = 0.02469079\n",
      "Iteration 9, loss = 0.02158208\n",
      "Iteration 10, loss = 0.01918025\n",
      "Iteration 11, loss = 0.01727484\n",
      "Iteration 12, loss = 0.01573020\n",
      "Iteration 13, loss = 0.01445540\n",
      "Iteration 14, loss = 0.01338750\n",
      "Iteration 15, loss = 0.01248127\n",
      "Iteration 16, loss = 0.01170414\n",
      "Iteration 17, loss = 0.01103129\n",
      "Iteration 18, loss = 0.01044391\n",
      "Iteration 19, loss = 0.00992736\n",
      "Iteration 20, loss = 0.00947021\n",
      "Iteration 21, loss = 0.00906319\n",
      "Iteration 22, loss = 0.00869875\n",
      "Iteration 23, loss = 0.00837090\n",
      "Iteration 24, loss = 0.00807457\n",
      "Iteration 25, loss = 0.00780566\n",
      "Iteration 26, loss = 0.00756068\n",
      "Iteration 27, loss = 0.00733670\n",
      "Iteration 28, loss = 0.00713129\n",
      "Iteration 29, loss = 0.00694224\n",
      "Iteration 30, loss = 0.00676777\n",
      "Iteration 31, loss = 0.00660630\n",
      "Iteration 32, loss = 0.00645650\n",
      "Iteration 33, loss = 0.00631709\n",
      "Iteration 34, loss = 0.00618725\n",
      "Iteration 35, loss = 0.00606589\n",
      "Iteration 36, loss = 0.00595222\n",
      "Iteration 37, loss = 0.00584559\n",
      "Iteration 38, loss = 0.00574544\n",
      "Iteration 39, loss = 0.00565107\n",
      "Iteration 40, loss = 0.00556217\n",
      "Iteration 41, loss = 0.00547824\n",
      "Iteration 42, loss = 0.00539886\n",
      "Iteration 43, loss = 0.00532365\n",
      "Iteration 44, loss = 0.00525236\n",
      "Iteration 45, loss = 0.00518473\n",
      "Iteration 46, loss = 0.00512040\n",
      "Iteration 47, loss = 0.00505919\n",
      "Iteration 48, loss = 0.00500098\n",
      "Iteration 49, loss = 0.00494541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000020\n",
      "Iteration 50, loss = 0.00491240\n",
      "Iteration 51, loss = 0.00490190\n",
      "Iteration 52, loss = 0.00489151\n",
      "Iteration 53, loss = 0.00488120\n",
      "Iteration 54, loss = 0.00487098\n",
      "Iteration 55, loss = 0.00486087\n",
      "Iteration 56, loss = 0.00485084\n",
      "Iteration 57, loss = 0.00484090\n",
      "Iteration 58, loss = 0.00483105\n",
      "Iteration 59, loss = 0.00482129\n",
      "Iteration 60, loss = 0.00481162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000004\n",
      "Iteration 61, loss = 0.00480569\n",
      "Iteration 62, loss = 0.00480377\n",
      "Iteration 63, loss = 0.00480186\n",
      "Iteration 64, loss = 0.00479996\n",
      "Iteration 65, loss = 0.00479805\n",
      "Iteration 66, loss = 0.00479615\n",
      "Iteration 67, loss = 0.00479425\n",
      "Iteration 68, loss = 0.00479236\n",
      "Iteration 69, loss = 0.00479047\n",
      "Iteration 70, loss = 0.00478858\n",
      "Iteration 71, loss = 0.00478670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 72, loss = 0.00478553\n",
      "Iteration 73, loss = 0.00478516\n",
      "Iteration 74, loss = 0.00478478\n",
      "Iteration 75, loss = 0.00478441\n",
      "Iteration 76, loss = 0.00478403\n",
      "Iteration 77, loss = 0.00478366\n",
      "Iteration 78, loss = 0.00478328\n",
      "Iteration 79, loss = 0.00478290\n",
      "Iteration 80, loss = 0.00478253\n",
      "Iteration 81, loss = 0.00478215\n",
      "Iteration 82, loss = 0.00478178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 2, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           2\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init      0.0001\n",
      "learning_rate         adaptive\n",
      "iterations                  82\n",
      "train_score           0.999368\n",
      "val_score             0.999298\n",
      "test_score            0.999175\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "observations_df = pd.DataFrame()\n",
    "\n",
    "for index,hyper_param_option in enumerate(hyper_param_options):\n",
    "    print(f\"Training model {index}, with hyper params: {hyper_param_option}\")\n",
    "    result = train_model(index, hyper_param_option)\n",
    "    result_df = pd.DataFrame([result])\n",
    "    observations_df = pd.concat([observations_df, result_df], ignore_index=True)  \n",
    "    print(f\"Training model {index}, with hyper params: {hyper_param_option} completed\")\n",
    "    print(\"Test results: \", observations_df.iloc[index])\n",
    "\n",
    "observations_df.to_csv('./data/observations.csv', mode='a',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>architecture</th>\n",
       "      <th>activation</th>\n",
       "      <th>solver</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>82</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.999508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>77</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>82</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hidden_layers  architecture activation solver  batch_size  \\\n",
       "0   0              1            30       tanh    sgd         100   \n",
       "1   1              1            30       tanh    sgd         100   \n",
       "2   2              1            30       tanh    sgd         100   \n",
       "\n",
       "   learning_rate_init learning_rate  iterations  train_score  val_score  \\\n",
       "0              0.0100      adaptive          82     0.999594   0.999438   \n",
       "1              0.0010      adaptive          77     0.999473   0.999368   \n",
       "2              0.0001      adaptive          82     0.999368   0.999298   \n",
       "\n",
       "   test_score  \n",
       "0    0.999508  \n",
       "1    0.999350  \n",
       "2    0.999175  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
