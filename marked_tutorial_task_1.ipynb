{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "# add more col name to columns\n",
    "columns.insert(0, 'time')\n",
    "columns.append('amount')\n",
    "columns.append('target')\n",
    "columns\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('./data/card.csv', names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>amount</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  amount  target  \n",
       "0 -0.189115  0.133558 -0.021053  149.62       0  \n",
       "1  0.125895 -0.008983  0.014724    2.69       0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66       0  \n",
       "3 -0.221929  0.062723  0.061458  123.50       0  \n",
       "4  0.502292  0.219422  0.215153   69.99       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictor and traget variables\n",
    "\n",
    "X = df.loc[:, 'time':'amount']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284807, 30), (284807,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape pf the data\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data into train_validate and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(1/8), random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199364, 30), (28481, 30), (56962, 30), (199364,), (28481,), (56962,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the data\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyper_param_options = [\n",
    "    # activation function relu\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # # activation function tanh\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (90), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # change the acrhitecture\n",
    "    # {'hidden_layer_sizes': (60, 30), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20, 10), 'activation': 'relu', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (60, 30, 20, 10), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True}, \n",
    "    # {'hidden_layer_sizes': (32, 18, 6, 2), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.001,   'random_state': 33,  'verbose': True}\n",
    "\n",
    "    # change initial learning rate with activation function tanh, batch size 50 and architecture 32\n",
    "    # {'hidden_layer_sizes': (20), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (28), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (32), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (36), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "\n",
    "    # best params so far with activation function tanh, batch size 100, learning_rate_init 0.0001 and architecture 32\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 50, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 75, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 150, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "    # {'hidden_layer_sizes': (30), 'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate_init': 0.0001,   'random_state': 33,  'verbose': True},\n",
    "\n",
    "    # best params so far with activation function tanh, batch size 100, learning_rate_init 0.001, solver sgd, learning_rate adaptive and architecture 30\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "    {'hidden_layer_sizes': (30), 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33,  'verbose': True},\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def train_model(id, hyper_params):\n",
    "    mlp_classifier = MLPClassifier(**hyper_params).fit(X_train_scaled, y_train)\n",
    "    train_score = mlp_classifier.score(X_train_scaled, y_train)\n",
    "    val_score = mlp_classifier.score(X_val_scaled, y_val)\n",
    "    test_score = mlp_classifier.score(X_test_scaled, y_test)\n",
    "    return {\n",
    "        'id': id,\n",
    "        'hidden_layers': mlp_classifier.n_layers_ - 2,\n",
    "        'architecture': mlp_classifier.hidden_layer_sizes,\n",
    "        'activation': hyper_params['activation'],\n",
    "        'solver': hyper_params['solver'],\n",
    "        'batch_size': hyper_params['batch_size'],\n",
    "        'learning_rate_init': hyper_params['learning_rate_init'],\n",
    "        'learning_rate': hyper_params['learning_rate'],\n",
    "        'iterations': mlp_classifier.n_iter_,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.01339956\n",
      "Iteration 2, loss = 0.00339689\n",
      "Iteration 3, loss = 0.00308715\n",
      "Iteration 4, loss = 0.00292665\n",
      "Iteration 5, loss = 0.00278040\n",
      "Iteration 6, loss = 0.00269872\n",
      "Iteration 7, loss = 0.00263662\n",
      "Iteration 8, loss = 0.00258221\n",
      "Iteration 9, loss = 0.00254363\n",
      "Iteration 10, loss = 0.00250517\n",
      "Iteration 11, loss = 0.00247824\n",
      "Iteration 12, loss = 0.00243345\n",
      "Iteration 13, loss = 0.00241175\n",
      "Iteration 14, loss = 0.00238691\n",
      "Iteration 15, loss = 0.00237011\n",
      "Iteration 16, loss = 0.00234250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 17, loss = 0.00229375\n",
      "Iteration 18, loss = 0.00228691\n",
      "Iteration 19, loss = 0.00228229\n",
      "Iteration 20, loss = 0.00227835\n",
      "Iteration 21, loss = 0.00227499\n",
      "Iteration 22, loss = 0.00227073\n",
      "Iteration 23, loss = 0.00226701\n",
      "Iteration 24, loss = 0.00226398\n",
      "Iteration 25, loss = 0.00226003\n",
      "Iteration 26, loss = 0.00225535\n",
      "Iteration 27, loss = 0.00225369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 28, loss = 0.00224118\n",
      "Iteration 29, loss = 0.00224044\n",
      "Iteration 30, loss = 0.00223980\n",
      "Iteration 31, loss = 0.00223897\n",
      "Iteration 32, loss = 0.00223830\n",
      "Iteration 33, loss = 0.00223760\n",
      "Iteration 34, loss = 0.00223690\n",
      "Iteration 35, loss = 0.00223632\n",
      "Iteration 36, loss = 0.00223548\n",
      "Iteration 37, loss = 0.00223491\n",
      "Iteration 38, loss = 0.00223434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 39, loss = 0.00223172\n",
      "Iteration 40, loss = 0.00223159\n",
      "Iteration 41, loss = 0.00223145\n",
      "Iteration 42, loss = 0.00223130\n",
      "Iteration 43, loss = 0.00223116\n",
      "Iteration 44, loss = 0.00223103\n",
      "Iteration 45, loss = 0.00223091\n",
      "Iteration 46, loss = 0.00223076\n",
      "Iteration 47, loss = 0.00223063\n",
      "Iteration 48, loss = 0.00223049\n",
      "Iteration 49, loss = 0.00223035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "Iteration 50, loss = 0.00222984\n",
      "Iteration 51, loss = 0.00222982\n",
      "Iteration 52, loss = 0.00222979\n",
      "Iteration 53, loss = 0.00222977\n",
      "Iteration 54, loss = 0.00222973\n",
      "Iteration 55, loss = 0.00222971\n",
      "Iteration 56, loss = 0.00222969\n",
      "Iteration 57, loss = 0.00222965\n",
      "Iteration 58, loss = 0.00222963\n",
      "Iteration 59, loss = 0.00222960\n",
      "Iteration 60, loss = 0.00222957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000003\n",
      "Iteration 61, loss = 0.00222947\n",
      "Iteration 62, loss = 0.00222946\n",
      "Iteration 63, loss = 0.00222946\n",
      "Iteration 64, loss = 0.00222945\n",
      "Iteration 65, loss = 0.00222945\n",
      "Iteration 66, loss = 0.00222944\n",
      "Iteration 67, loss = 0.00222944\n",
      "Iteration 68, loss = 0.00222943\n",
      "Iteration 69, loss = 0.00222943\n",
      "Iteration 70, loss = 0.00222942\n",
      "Iteration 71, loss = 0.00222942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 72, loss = 0.00222939\n",
      "Iteration 73, loss = 0.00222939\n",
      "Iteration 74, loss = 0.00222939\n",
      "Iteration 75, loss = 0.00222939\n",
      "Iteration 76, loss = 0.00222939\n",
      "Iteration 77, loss = 0.00222939\n",
      "Iteration 78, loss = 0.00222939\n",
      "Iteration 79, loss = 0.00222939\n",
      "Iteration 80, loss = 0.00222939\n",
      "Iteration 81, loss = 0.00222938\n",
      "Iteration 82, loss = 0.00222938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 0, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           0\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init        0.01\n",
      "learning_rate         adaptive\n",
      "iterations                  82\n",
      "train_score           0.999594\n",
      "val_score             0.999438\n",
      "test_score            0.999508\n",
      "Name: 0, dtype: object\n",
      "Training model 1, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.07498791\n",
      "Iteration 2, loss = 0.01256333\n",
      "Iteration 3, loss = 0.00776980\n",
      "Iteration 4, loss = 0.00604111\n",
      "Iteration 5, loss = 0.00516973\n",
      "Iteration 6, loss = 0.00465278\n",
      "Iteration 7, loss = 0.00431198\n",
      "Iteration 8, loss = 0.00407061\n",
      "Iteration 9, loss = 0.00389062\n",
      "Iteration 10, loss = 0.00374895\n",
      "Iteration 11, loss = 0.00363354\n",
      "Iteration 12, loss = 0.00353650\n",
      "Iteration 13, loss = 0.00345526\n",
      "Iteration 14, loss = 0.00338395\n",
      "Iteration 15, loss = 0.00332057\n",
      "Iteration 16, loss = 0.00326392\n",
      "Iteration 17, loss = 0.00321567\n",
      "Iteration 18, loss = 0.00317293\n",
      "Iteration 19, loss = 0.00313603\n",
      "Iteration 20, loss = 0.00310381\n",
      "Iteration 21, loss = 0.00307536\n",
      "Iteration 22, loss = 0.00304993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 23, loss = 0.00303032\n",
      "Iteration 24, loss = 0.00302570\n",
      "Iteration 25, loss = 0.00302123\n",
      "Iteration 26, loss = 0.00301681\n",
      "Iteration 27, loss = 0.00301256\n",
      "Iteration 28, loss = 0.00300829\n",
      "Iteration 29, loss = 0.00300411\n",
      "Iteration 30, loss = 0.00300000\n",
      "Iteration 31, loss = 0.00299597\n",
      "Iteration 32, loss = 0.00299209\n",
      "Iteration 33, loss = 0.00298815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 34, loss = 0.00298469\n",
      "Iteration 35, loss = 0.00298394\n",
      "Iteration 36, loss = 0.00298316\n",
      "Iteration 37, loss = 0.00298240\n",
      "Iteration 38, loss = 0.00298166\n",
      "Iteration 39, loss = 0.00298090\n",
      "Iteration 40, loss = 0.00298015\n",
      "Iteration 41, loss = 0.00297939\n",
      "Iteration 42, loss = 0.00297865\n",
      "Iteration 43, loss = 0.00297790\n",
      "Iteration 44, loss = 0.00297716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 45, loss = 0.00297648\n",
      "Iteration 46, loss = 0.00297633\n",
      "Iteration 47, loss = 0.00297618\n",
      "Iteration 48, loss = 0.00297604\n",
      "Iteration 49, loss = 0.00297589\n",
      "Iteration 50, loss = 0.00297574\n",
      "Iteration 51, loss = 0.00297559\n",
      "Iteration 52, loss = 0.00297545\n",
      "Iteration 53, loss = 0.00297530\n",
      "Iteration 54, loss = 0.00297515\n",
      "Iteration 55, loss = 0.00297500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 56, loss = 0.00297487\n",
      "Iteration 57, loss = 0.00297484\n",
      "Iteration 58, loss = 0.00297481\n",
      "Iteration 59, loss = 0.00297478\n",
      "Iteration 60, loss = 0.00297475\n",
      "Iteration 61, loss = 0.00297472\n",
      "Iteration 62, loss = 0.00297469\n",
      "Iteration 63, loss = 0.00297466\n",
      "Iteration 64, loss = 0.00297463\n",
      "Iteration 65, loss = 0.00297460\n",
      "Iteration 66, loss = 0.00297457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 67, loss = 0.00297455\n",
      "Iteration 68, loss = 0.00297454\n",
      "Iteration 69, loss = 0.00297453\n",
      "Iteration 70, loss = 0.00297453\n",
      "Iteration 71, loss = 0.00297452\n",
      "Iteration 72, loss = 0.00297452\n",
      "Iteration 73, loss = 0.00297451\n",
      "Iteration 74, loss = 0.00297450\n",
      "Iteration 75, loss = 0.00297450\n",
      "Iteration 76, loss = 0.00297449\n",
      "Iteration 77, loss = 0.00297449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 1, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           1\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init       0.001\n",
      "learning_rate         adaptive\n",
      "iterations                  77\n",
      "train_score           0.999473\n",
      "val_score             0.999368\n",
      "test_score             0.99935\n",
      "Name: 1, dtype: object\n",
      "Training model 2, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True}\n",
      "Iteration 1, loss = 0.29704887\n",
      "Iteration 2, loss = 0.13460608\n",
      "Iteration 3, loss = 0.08152229\n",
      "Iteration 4, loss = 0.05693445\n",
      "Iteration 5, loss = 0.04322947\n",
      "Iteration 6, loss = 0.03465433\n",
      "Iteration 7, loss = 0.02884975\n",
      "Iteration 8, loss = 0.02469079\n",
      "Iteration 9, loss = 0.02158208\n",
      "Iteration 10, loss = 0.01918025\n",
      "Iteration 11, loss = 0.01727484\n",
      "Iteration 12, loss = 0.01573020\n",
      "Iteration 13, loss = 0.01445540\n",
      "Iteration 14, loss = 0.01338750\n",
      "Iteration 15, loss = 0.01248127\n",
      "Iteration 16, loss = 0.01170414\n",
      "Iteration 17, loss = 0.01103129\n",
      "Iteration 18, loss = 0.01044391\n",
      "Iteration 19, loss = 0.00992736\n",
      "Iteration 20, loss = 0.00947021\n",
      "Iteration 21, loss = 0.00906319\n",
      "Iteration 22, loss = 0.00869875\n",
      "Iteration 23, loss = 0.00837090\n",
      "Iteration 24, loss = 0.00807457\n",
      "Iteration 25, loss = 0.00780566\n",
      "Iteration 26, loss = 0.00756068\n",
      "Iteration 27, loss = 0.00733670\n",
      "Iteration 28, loss = 0.00713129\n",
      "Iteration 29, loss = 0.00694224\n",
      "Iteration 30, loss = 0.00676777\n",
      "Iteration 31, loss = 0.00660630\n",
      "Iteration 32, loss = 0.00645650\n",
      "Iteration 33, loss = 0.00631709\n",
      "Iteration 34, loss = 0.00618725\n",
      "Iteration 35, loss = 0.00606589\n",
      "Iteration 36, loss = 0.00595222\n",
      "Iteration 37, loss = 0.00584559\n",
      "Iteration 38, loss = 0.00574544\n",
      "Iteration 39, loss = 0.00565107\n",
      "Iteration 40, loss = 0.00556217\n",
      "Iteration 41, loss = 0.00547824\n",
      "Iteration 42, loss = 0.00539886\n",
      "Iteration 43, loss = 0.00532365\n",
      "Iteration 44, loss = 0.00525236\n",
      "Iteration 45, loss = 0.00518473\n",
      "Iteration 46, loss = 0.00512040\n",
      "Iteration 47, loss = 0.00505919\n",
      "Iteration 48, loss = 0.00500098\n",
      "Iteration 49, loss = 0.00494541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000020\n",
      "Iteration 50, loss = 0.00491240\n",
      "Iteration 51, loss = 0.00490190\n",
      "Iteration 52, loss = 0.00489151\n",
      "Iteration 53, loss = 0.00488120\n",
      "Iteration 54, loss = 0.00487098\n",
      "Iteration 55, loss = 0.00486087\n",
      "Iteration 56, loss = 0.00485084\n",
      "Iteration 57, loss = 0.00484090\n",
      "Iteration 58, loss = 0.00483105\n",
      "Iteration 59, loss = 0.00482129\n",
      "Iteration 60, loss = 0.00481162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000004\n",
      "Iteration 61, loss = 0.00480569\n",
      "Iteration 62, loss = 0.00480377\n",
      "Iteration 63, loss = 0.00480186\n",
      "Iteration 64, loss = 0.00479996\n",
      "Iteration 65, loss = 0.00479805\n",
      "Iteration 66, loss = 0.00479615\n",
      "Iteration 67, loss = 0.00479425\n",
      "Iteration 68, loss = 0.00479236\n",
      "Iteration 69, loss = 0.00479047\n",
      "Iteration 70, loss = 0.00478858\n",
      "Iteration 71, loss = 0.00478670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 72, loss = 0.00478553\n",
      "Iteration 73, loss = 0.00478516\n",
      "Iteration 74, loss = 0.00478478\n",
      "Iteration 75, loss = 0.00478441\n",
      "Iteration 76, loss = 0.00478403\n",
      "Iteration 77, loss = 0.00478366\n",
      "Iteration 78, loss = 0.00478328\n",
      "Iteration 79, loss = 0.00478290\n",
      "Iteration 80, loss = 0.00478253\n",
      "Iteration 81, loss = 0.00478215\n",
      "Iteration 82, loss = 0.00478178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "Training model 2, with hyper params: {'hidden_layer_sizes': 30, 'activation': 'tanh', 'batch_size': 100, 'solver': 'sgd', 'learning_rate_init': 0.0001, 'learning_rate': 'adaptive', 'random_state': 33, 'verbose': True} completed\n",
      "Test results:  id                           2\n",
      "hidden_layers                1\n",
      "architecture                30\n",
      "activation                tanh\n",
      "solver                     sgd\n",
      "batch_size                 100\n",
      "learning_rate_init      0.0001\n",
      "learning_rate         adaptive\n",
      "iterations                  82\n",
      "train_score           0.999368\n",
      "val_score             0.999298\n",
      "test_score            0.999175\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "observations_df = pd.DataFrame()\n",
    "\n",
    "for index,hyper_param_option in enumerate(hyper_param_options):\n",
    "    print(f\"Training model {index}, with hyper params: {hyper_param_option}\")\n",
    "    result = train_model(index, hyper_param_option)\n",
    "    result_df = pd.DataFrame([result])\n",
    "    observations_df = pd.concat([observations_df, result_df], ignore_index=True)  \n",
    "    print(f\"Training model {index}, with hyper params: {hyper_param_option} completed\")\n",
    "    print(\"Test results: \", observations_df.iloc[index])\n",
    "\n",
    "observations_df.to_csv('./data/observations.csv', mode='a',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>architecture</th>\n",
       "      <th>activation</th>\n",
       "      <th>solver</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>iterations</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>82</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>0.999508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>77</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>82</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  hidden_layers  architecture activation solver  batch_size  \\\n",
       "0   0              1            30       tanh    sgd         100   \n",
       "1   1              1            30       tanh    sgd         100   \n",
       "2   2              1            30       tanh    sgd         100   \n",
       "\n",
       "   learning_rate_init learning_rate  iterations  train_score  val_score  \\\n",
       "0              0.0100      adaptive          82     0.999594   0.999438   \n",
       "1              0.0010      adaptive          77     0.999473   0.999368   \n",
       "2              0.0001      adaptive          82     0.999368   0.999298   \n",
       "\n",
       "   test_score  \n",
       "0    0.999508  \n",
       "1    0.999350  \n",
       "2    0.999175  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.03331534\n",
      "Iteration 2, loss = 0.00309743\n",
      "Iteration 3, loss = 0.00272658\n",
      "Iteration 4, loss = 0.00255233\n",
      "Iteration 5, loss = 0.00248779\n",
      "Iteration 6, loss = 0.00238446\n",
      "Iteration 7, loss = 0.00227328\n",
      "Iteration 8, loss = 0.00222404\n",
      "Iteration 9, loss = 0.00209071\n",
      "Iteration 10, loss = 0.00203846\n",
      "Iteration 11, loss = 0.00201007\n",
      "Iteration 12, loss = 0.00190546\n",
      "Iteration 13, loss = 0.00185026\n",
      "Iteration 14, loss = 0.00182062\n",
      "Iteration 15, loss = 0.00178613\n",
      "Iteration 16, loss = 0.00168027\n",
      "Iteration 17, loss = 0.00162781\n",
      "Iteration 18, loss = 0.00158216\n",
      "Iteration 19, loss = 0.00155894\n",
      "Iteration 20, loss = 0.00147610\n",
      "Iteration 21, loss = 0.00142374\n",
      "Iteration 22, loss = 0.00139327\n",
      "Iteration 23, loss = 0.00136684\n",
      "Iteration 24, loss = 0.00130610\n",
      "Iteration 25, loss = 0.00128075\n",
      "Iteration 26, loss = 0.00122158\n",
      "Iteration 27, loss = 0.00119432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': (30), \n",
    "    'activation': 'tanh', \n",
    "    'alpha': 0.0001, \n",
    "    'batch_size': 100, \n",
    "    'learning_rate_init': 0.001,  \n",
    "    'random_state': 33,  \n",
    "    'verbose': True\n",
    "    }\n",
    "\n",
    "mlp_classifier = MLPClassifier(**params).fit(X_train_scaled, y_train)\n",
    "y_pred = mlp_classifier.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAGdCAYAAAC/02HYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAu60lEQVR4nO3dfXhU9Z3//9eQkDGEZMwNyTAaFDVNoYm0RBsCKigQoIRI7RZq3FloKTeNErNJFpe6u6XfX5tUQLDdbC3FrljFTX9bCrUFYrC1aMp9MJYgWlFaQDIEZEhIjDMhzPcP17PfOeEmoeeYIM+H17kuc847Zz7DdWFevt+fM3GEQqGQAAAALNavtxcAAAA+nQgZAADAFoQMAABgC0IGAACwBSEDAADYgpABAABsQcgAAAC2IGQAAABbEDIAAIAtInt7AR/LHFna20sA+pzFTw3p7SUAfVLByIdtvb+VP5P27X3csntdaehkAAAAWxAyAACALfrMuAQAgD7D0dsL+HQgZAAAYOYgZViBkAEAgBkZwxLsyQAAALagkwEAgBmdDEsQMgAA6IKUYQXGJQAAwBZ0MgAAMAnRyLAEIQMAADNChiUYlwAAAFvQyQAAwIwP47IEnQwAAGALQgYAALAF4xIAAMyYlliCkAEAgBl7MixByAAAwIyMYQn2ZAAAAFvQyQAAwCTU2wv4lCBkAABgxp4MSzAuAQAAtqCTAQCAGY0MSxAyAADogpRhBcYlAADAFnQyAAAwo5FhCUIGAABmhAxLMC4BAAC2oJMBAIBJiM/JsASdDAAAYAs6GQAAmNHJsASdDAAAYAs6GQAAmNHIsAQhAwAAE34LqzUYlwAAAFvQyQAAwIyNn5YgZAAAYEbGsATjEgAAYAs6GQAAmDEusQSdDAAATEIWHj2xZMkSORyOsMPtdv/vukIhLVmyRB6PR9HR0Ro3bpz2798fdo9AIKCFCxcqKSlJMTExys/P19GjR8Nq/H6/vF6vXC6XXC6XvF6vTp8+HVZz+PBhTZs2TTExMUpKSlJRUZGCwWCP3g8hAwCAPuRzn/ucGhsbjWPfvn3GtaVLl2rFihWqrKzU7t275Xa7NXHiRJ05c8aoKS4u1vr161VVVaXa2lq1trYqLy9PnZ2dRk1BQYHq6+tVXV2t6upq1dfXy+v1Gtc7Ozs1depUtbW1qba2VlVVVVq3bp1KS0t79F4YlwAAYNaL05LIyMiw7sXHQqGQnnjiCT366KO67777JEnPPPOMUlJS9Pzzz2v+/Plqbm7Wz372Mz377LOaMGGCJOm5555TamqqXnrpJU2aNEkHDhxQdXW1duzYoezsbEnS6tWrlZOTo7feekvp6emqqanRG2+8oSNHjsjj8UiSHn/8cc2ePVvf//73FRcX1633QicDAAAzh8OyIxAIqKWlJewIBAIXfOm3335bHo9HQ4cO1de+9jW9++67kqRDhw7J5/MpNzfXqHU6nRo7dqy2bdsmSaqrq1NHR0dYjcfjUUZGhlGzfft2uVwuI2BI0qhRo+RyucJqMjIyjIAhSZMmTVIgEFBdXV23/xgJGQAA2KiiosLY+/DxUVFRcd7a7Oxs/fznP9eLL76o1atXy+fzafTo0Xr//ffl8/kkSSkpKWHfk5KSYlzz+XyKiopSfHz8RWuSk5O7vHZycnJYjfl14uPjFRUVZdR0B+MSAABstHjxYpWUlISdczqd562dMmWK8e+ZmZnKycnRzTffrGeeeUajRo2SJDlMT76EQqEu58zMNeerv5yaS6GTAQCAScjhsOxwOp2Ki4sLOy4UMsxiYmKUmZmpt99+29inYe4kNDU1GV0Ht9utYDAov99/0Zrjx493ea0TJ06E1Zhfx+/3q6Ojo0uH42IIGQAAmDksPP4GgUBABw4c0ODBgzV06FC53W5t2bLFuB4MBrV161aNHj1akpSVlaX+/fuH1TQ2NqqhocGoycnJUXNzs3bt2mXU7Ny5U83NzWE1DQ0NamxsNGpqamrkdDqVlZXV7fUzLgEAoI8oKyvTtGnTNGTIEDU1Nel73/ueWlpaNGvWLDkcDhUXF6u8vFxpaWlKS0tTeXm5BgwYoIKCAkmSy+XSnDlzVFpaqsTERCUkJKisrEyZmZnG0ybDhg3T5MmTNXfuXK1atUqSNG/ePOXl5Sk9PV2SlJubq+HDh8vr9WrZsmU6deqUysrKNHfu3G4/WSIRMgAA6DOOHj2q+++/XydPntSgQYM0atQo7dixQzfccIMkadGiRWpvb1dhYaH8fr+ys7NVU1Oj2NhY4x4rV65UZGSkZsyYofb2do0fP15r1qxRRESEUbN27VoVFRUZT6Hk5+ersrLSuB4REaGNGzeqsLBQY8aMUXR0tAoKCrR8+fIevR9HKBTq6QeS2SJzZM8+4AO4Gix+akhvLwHokwpGPmzr/T8zdYll9/rzRuvudaVhTwYAALAF4xIAAMz4/WiWIGQAAGBGyLAE4xIAAGALOhkAAHRBK8MKhAwAAExCZAxLEDIAADAjZFiCPRkAAMAWdDIAAOiCVoYVCBkAAJiwJ8MajEsAAIAt6GQAAGBGJ8MShAwAALogZViBcQkAALAFnQwAAEzY+GkNQgYAAGaEDEswLgEAALagkwEAQBe0MqxAyAAAwIyMYQlCBgAAJmz8tAZ7MgAAgC3oZAAAYEYnwxKEDAAAuiBlWIFxCQAAsAWdDAAATNj4aQ1CBgAAZoQMSzAuAQAAtiBkAAAAWzAuAQDAzMG8xAp0MgAAgC3oZAAAYMLTJdagkwEAAGxBJwMAADM6GZagkwEAAGxBJwMAADOeLrEEIaOP+9b8XBXOnxR27uTJFt2d+93z1n9vydd0b/7tXc4ffMenL391mS1rlKS0W9z69iP3KeNzQ9Tc8oF+uW67frJ6y3lrPz/iRj29ulAH3/Hpq/evsG1NgBVaTrXqpee36+Drh9UR7FTiYJfy590tz03Jvb002CjU2wv4lCBkXAHePtioud9aZXx9rvPcBWt/sHyDVv77RuPryIh++mVVqWpeev2yX98zOF4vbvwXZY4sPe/1mBinfvrj+dq15x3d731CN9wwSN9b8jV90B7Uz5/bGlY7cOA1Kv8/92vn7oNKTBh42WsCPgntrR/qP7+zXkM/d50eeCRPMa5onTreomtinL29NOCKQMi4AnR2ntP775/pVm1r64dqbf3Q+PqecRmKi4vWhhd2h9VNz79dX591t67zJOjYMb/WVr2qX/z3tsta39QpIxXl7K9/+c5/qaOjUwff8enGIYP0D38/tkvI+LdH/06bql9T57lzumdcxmW9HvBJ+eNvXpMrcaDuXXCPce7aQXG9uCJ8YpiWWIKQcQUYMiRJv3vx3xQMntW+hsP6UeUmHX3vVLe+98vTv6gdO99WY6PfOPeVL2ercMEklT+2Xm+++Z4++9nrtORfvqr29qBe+O2eHq9vxK03qq7uHXV0dBrn/rj9LRUXTdV1ngS9d+yjtU7Pv12p1ydq8b88r3nfnNDj1wE+aW/V/UU335qq/37iRf3lwDHFxcfotokZyho/vLeXBrsRMizR45Bx9OhRPfnkk9q2bZt8Pp8cDodSUlI0evRoLViwQKmpqXas86q1b99hPfqv/6W/Hj6hxIRYzfvmBD379EJN/+oyNTd/cNHvTUqK1R2jP6t/fnRt2Pn535yo5St+o9/9fp8k6b1jp3Tz0BR99Ss5lxUykhJjdawxPPR83HlJSorVe8dOaUhqkooXTtWsOZXqvMi4B+hL/E0t2vPSfuV8aYTuuHek3nunSdXPvKrI/v004q7P9vbygD6vRyGjtrZWU6ZMUWpqqnJzc5Wbm6tQKKSmpiZt2LBB//7v/67NmzdrzJgxF71PIBBQIBAIO3fu3Fn160djxax225vGv78tn17/01+16YXFujfvNv187SsX/d57p92uM2c+1O9ebjDOxV8bo8GD4/Xdf5uhJf/6VeN8RES/sDHL+v/+J3kGx3/0xf8k+p215cb1Y43+sI2kIdMuqY83ZodCIfXr59Bj5Q/oP37yov56+GS33jfQF4TOheS5aZDGf22UJGnw0EE6cfSU9ry0n5ABdEOPfqr/4z/+o775zW9q5cqVF7xeXFys3bt3n/f6xyoqKvTd74Y/HTHIPUopg0f3ZDlXpfYPg3r7oE9Dhgy6ZO2X7/2ifrtpj86e/d8xRr9+H/30/+73/lt/avhrWP25zv9NCoVFTyky8qOPUUkZ5NLTTz2ov7v/ceP62bP/2404+f4ZJSXGht0rIeGjr99/v1UxA5zK+NwQfTb9On37kS8b6+jXr59e27VU8x/8qXbtPtit9w98kmLjB2jQ9Qlh55Kui9eBXe/20orwieERVkv0KGQ0NDToueeeu+D1+fPn6yc/+ckl77N48WKVlJSEncu56197spSrVv/+EbppaLL2vnbx/8jdlnWzbhgySL/asCvs/PunWnX8+Gldf12CNm7ee8Hv/3/3cHT+T6A4cuT989a+/qe/6OGHvqTIyAgj0Iwe9Rkdb2rWe8dOyeFwdHl8duZXRyv79jSVLHpG73VzfwnwSUv9zGC9f+x02Ln3G0/LlcSTUUB39OgTPwcPHqxt2y78BML27ds1ePDgS97H6XQqLi4u7GBUcn6lxdN028ibdJ0nQZkZQ7Ri2SzFxFyjX//P3omHH/qSvv9/7u/yffdN/6Je3/dXHXzH1+Xaj1fVaM7Xx+uB++/UDUOSlHaLW9Pzb9c/PHDXZa1xU/VrCgbP6vvf/Zpuudmte+7O0De/Md54siQUCungO76w45S/VYFghw6+41P7h8HLel3AbqO+dKuOHjyuVzfU6ZSvWfv++Gft/f0buj03s7eXBpuFHNYdV7Me/WQvKyvTggULVFdXp4kTJyolJUUOh0M+n09btmzRU089pSeeeMKmpV6dUlJceqzi7xV/bYxO+dv0p31/1QOzfmR0GgYlxWmw+9qw7xk48BpNuOdWPbZ8w3nv+asNO/Xhh0HN/oe7VfJwntrbg3r7YKOeff7iezwupLX1Q80rXKVH//k+VT1XrJaWdv187StdHl8FrjTX3ZyimSWT9buqHdr6qz2KHxSrSd47dOsdn+ntpQFXBEcoZN6yd3G/+MUvtHLlStXV1amz86PWeEREhLKyslRSUqIZM2Zc1kIu9EFPwNVs8VNDensJQJ9UMPJhW+8/5Os/sOxeh5/+Z8vudaXp8Yxi5syZmjlzpjo6OnTy5EdPCiQlJal///6WLw4AAFy5LnsjRP/+/bu1/wIAgCvOVb6XwirstgQAoAtShhV69HQJAABAd9HJAADAjEaGJQgZAACYETIswbgEAADYgk4GAAAmPfoAKVwQnQwAAMwcFh6XqaKiQg6HQ8XFxca5UCikJUuWyOPxKDo6WuPGjdP+/fvDvi8QCGjhwoVKSkpSTEyM8vPzdfTo0bAav98vr9crl8sll8slr9er06dPh9UcPnxY06ZNU0xMjJKSklRUVKRgsGe/BoKQAQBAH7N792799Kc/1a233hp2funSpVqxYoUqKyu1e/duud1uTZw4UWfOnDFqiouLtX79elVVVam2tlatra3Ky8szPqVbkgoKClRfX6/q6mpVV1ervr5eXq/XuN7Z2ampU6eqra1NtbW1qqqq0rp161Ra2rNP5yZkAADQh7S2tuqBBx7Q6tWrFR8fb5wPhUJ64okn9Oijj+q+++5TRkaGnnnmGX3wwQd6/vnnJUnNzc362c9+pscff1wTJkzQF77wBT333HPat2+fXnrpJUnSgQMHVF1draeeeko5OTnKycnR6tWr9dvf/lZvvfWWJKmmpkZvvPGGnnvuOX3hC1/QhAkT9Pjjj2v16tVqaWnp9nshZAAAYOZwWHYEAgG1tLSEHYFA4IIv/eCDD2rq1KmaMGFC2PlDhw7J5/MpNzfXOOd0OjV27FjjN6TX1dWpo6MjrMbj8SgjI8Oo2b59u1wul7Kzs42aUaNGyeVyhdVkZGTI4/EYNZMmTVIgEFBdXV23/xgJGQAAmFm4J6OiosLY+/DxUVFRcd6Xraqq0t69e8973efzSZJSUlLCzqekpBjXfD6foqKiwjog56tJTk7ucv/k5OSwGvPrxMfHKyoqyqjpDp4uAQDARosXL1ZJSUnYOafT2aXuyJEjevjhh1VTU6NrrrnmgvdzOMJ3k4ZCoS7nzMw156u/nJpLoZMBAICNnE6n4uLiwo7zhYy6ujo1NTUpKytLkZGRioyM1NatW/WjH/1IkZGRRmfB3EloamoyrrndbgWDQfn9/ovWHD9+vMvrnzhxIqzG/Dp+v18dHR1dOhwXQ8gAAMCsFx5hHT9+vPbt26f6+nrjuO222/TAAw+ovr5eN910k9xut7Zs2WJ8TzAY1NatWzV69GhJUlZWlvr37x9W09jYqIaGBqMmJydHzc3N2rVrl1Gzc+dONTc3h9U0NDSosbHRqKmpqZHT6VRWVla33xPjEgAA+oDY2FhlZGSEnYuJiVFiYqJxvri4WOXl5UpLS1NaWprKy8s1YMAAFRQUSJJcLpfmzJmj0tJSJSYmKiEhQWVlZcrMzDQ2kg4bNkyTJ0/W3LlztWrVKknSvHnzlJeXp/T0dElSbm6uhg8fLq/Xq2XLlunUqVMqKyvT3LlzFRcX1+33RMgAAMCsj/7ukkWLFqm9vV2FhYXy+/3Kzs5WTU2NYmNjjZqVK1cqMjJSM2bMUHt7u8aPH681a9YoIiLCqFm7dq2KioqMp1Dy8/NVWVlpXI+IiNDGjRtVWFioMWPGKDo6WgUFBVq+fHmP1usIhUJ94tNTM0f27AM+gKvB4qeG9PYSgD6pYOTDtt4/9VtLLbvXkScXWXavKw17MgAAgC0YlwAAYNZHxyVXGkIGAAAmPfksCFwY4xIAAGALQgYAALAF4xIAAMyYlliCkAEAgBkhwxKMSwAAgC0IGQAAwBaMSwAAMOEJVmvQyQAAALYgZAAAAFswLgEAwIxxiSXoZAAAAFvQyQAAwIxOhiUIGQAAmJAxrMG4BAAA2IJOBgAAZnxQhiUIGQAAmJAxrMG4BAAA2IKQAQAAbMG4BAAAM8YlliBkAABgQsawBuMSAABgCzoZAACY0cqwBCEDAAATHmG1BuMSAABgC0IGAACwBeMSAABMGJdYg04GAACwBSEDAADYgnEJAAAmjEusQcgAAMCMkGEJxiUAAMAWdDIAADBx0MqwBCEDAAAzMoYlGJcAAABb0MkAAMCERoY1CBkAAJjwCKs1GJcAAABb0MkAAMCMToYlCBkAAJiQMaxByAAAwIyUYQn2ZAAAAFvQyQAAwIRGhjUIGQAAmPAIqzUYlwAAAFvQyQAAwIxOhiUIGQAAmJAxrMG4BAAA2IJOBgAAJmz8tAadDAAAYAtCBgAAsAXjEgAATBiXWINOBgAAZg4Ljx548skndeuttyouLk5xcXHKycnR5s2bjeuhUEhLliyRx+NRdHS0xo0bp/3794fdIxAIaOHChUpKSlJMTIzy8/N19OjRsBq/3y+v1yuXyyWXyyWv16vTp0+H1Rw+fFjTpk1TTEyMkpKSVFRUpGAw2KP3Q8gAAMDEYeE/PXH99dfrBz/4gfbs2aM9e/bonnvu0b333msEiaVLl2rFihWqrKzU7t275Xa7NXHiRJ05c8a4R3FxsdavX6+qqirV1taqtbVVeXl56uzsNGoKCgpUX1+v6upqVVdXq76+Xl6v17je2dmpqVOnqq2tTbW1taqqqtK6detUWlrasz/HUCgU6tF32CRzZM8WDlwNFj81pLeXAPRJBSMftvX+I767wrJ7vf6dkr/p+xMSErRs2TJ94xvfkMfjUXFxsR555BFJH3UtUlJS9Nhjj2n+/Plqbm7WoEGD9Oyzz2rmzJmSpGPHjik1NVWbNm3SpEmTdODAAQ0fPlw7duxQdna2JGnHjh3KycnRm2++qfT0dG3evFl5eXk6cuSIPB6PJKmqqkqzZ89WU1OT4uLiurV2OhkAAJg4HNYdgUBALS0tYUcgELjkGjo7O1VVVaW2tjbl5OTo0KFD8vl8ys3NNWqcTqfGjh2rbdu2SZLq6urU0dERVuPxeJSRkWHUbN++XS6XywgYkjRq1Ci5XK6wmoyMDCNgSNKkSZMUCARUV1fX7T9HQgYAADaqqKgw9j58fFRUVFywft++fRo4cKCcTqcWLFig9evXa/jw4fL5fJKklJSUsPqUlBTjms/nU1RUlOLj4y9ak5yc3OV1k5OTw2rMrxMfH6+oqCijpjt4ugQAABstXrxYJSXhIxOn03nB+vT0dNXX1+v06dNat26dZs2apa1btxrXHaZHX0KhUJdzZuaa89VfTs2l0MkAAMDEynGJ0+k0nhb5+LhYyIiKitItt9yi2267TRUVFRoxYoR++MMfyu12S1KXTkJTU5PRdXC73QoGg/L7/RetOX78eJfXPXHiRFiN+XX8fr86Ojq6dDguhpABAIBJLz3Bel6hUEiBQEBDhw6V2+3Wli1bjGvBYFBbt27V6NGjJUlZWVnq379/WE1jY6MaGhqMmpycHDU3N2vXrl1Gzc6dO9Xc3BxW09DQoMbGRqOmpqZGTqdTWVlZ3V474xIAAPqIb3/725oyZYpSU1N15swZVVVV6Q9/+IOqq6vlcDhUXFys8vJypaWlKS0tTeXl5RowYIAKCgokSS6XS3PmzFFpaakSExOVkJCgsrIyZWZmasKECZKkYcOGafLkyZo7d65WrVolSZo3b57y8vKUnp4uScrNzdXw4cPl9Xq1bNkynTp1SmVlZZo7d263nyyRCBkAAHTVS5/4efz4cXm9XjU2NsrlcunWW29VdXW1Jk6cKElatGiR2tvbVVhYKL/fr+zsbNXU1Cg2Nta4x8qVKxUZGakZM2aovb1d48eP15o1axQREWHUrF27VkVFRcZTKPn5+aqsrDSuR0REaOPGjSosLNSYMWMUHR2tgoICLV++vEfvh8/JAPowPicDOD+7Pycj6/vWfU5G3aN/2+dkXMnYkwEAAGzBuAQAABN+P5o1CBkAAJiRMizRZ0IGs2cAQF9BxrAGezIAAIAt+kwnAwCAvqIHn5yNiyBkAABgRsiwBOMSAABgCzoZAACY0MiwBiEDAAAT9mRYg3EJAACwBZ0MAAC6oJVhBUIGAAAmjEuswbgEAADYgk4GAABmdDIsQcgAAMCEjGENQgYAACbsybAGezIAAIAtCBkAAMAWjEsAADBhXGINOhkAAMAWdDIAADChkWENQgYAAGakDEswLgEAALagkwEAgAkbP61ByAAAwISMYQ3GJQAAwBZ0MgAAMKOVYQlCBgAAJmQMaxAyAAAwYeOnNdiTAQAAbEEnAwAAM1oZliBkAABgQsSwBuMSAABgCzoZAACY0cqwBCEDAAATMoY1GJcAAABb0MkAAMCEh0usQcgAAMCMkGEJxiUAAMAWdDIAADChkWENQgYAACbsybAG4xIAAGALQgYAALAF4xIAAEwYl1iDkAEAgAkhwxqMSwAAgC0IGQAAwBaMSwAAMGFcYg06GQAAwBZ0MgAAMKGRYQ1CBgAAZqQMSzAuAQAAtqCTAQCACRs/rUEnAwAAE4eFR09UVFTo9ttvV2xsrJKTkzV9+nS99dZbYTWhUEhLliyRx+NRdHS0xo0bp/3794fVBAIBLVy4UElJSYqJiVF+fr6OHj0aVuP3++X1euVyueRyueT1enX69OmwmsOHD2vatGmKiYlRUlKSioqKFAwGu/1+CBkAAJg5HNYdPbB161Y9+OCD2rFjh7Zs2aKzZ88qNzdXbW1tRs3SpUu1YsUKVVZWavfu3XK73Zo4caLOnDlj1BQXF2v9+vWqqqpSbW2tWltblZeXp87OTqOmoKBA9fX1qq6uVnV1terr6+X1eo3rnZ2dmjp1qtra2lRbW6uqqiqtW7dOpaWl3f9jDIVCoR79Cdjk+b0/7O0lAACuEAUjH7b1/nlPWfcz6bffvPy1njhxQsnJydq6davuuusuhUIheTweFRcX65FHHpH0UdciJSVFjz32mObPn6/m5mYNGjRIzz77rGbOnClJOnbsmFJTU7Vp0yZNmjRJBw4c0PDhw7Vjxw5lZ2dLknbs2KGcnBy9+eabSk9P1+bNm5WXl6cjR47I4/FIkqqqqjR79mw1NTUpLi7ukuunkwEAgImV45JAIKCWlpawIxAIdGsdzc3NkqSEhARJ0qFDh+Tz+ZSbm2vUOJ1OjR07Vtu2bZMk1dXVqaOjI6zG4/EoIyPDqNm+fbtcLpcRMCRp1KhRcrlcYTUZGRlGwJCkSZMmKRAIqK6urlvrJ2QAAGBi5bSkoqLC2Pfw8VFRUXHJNYRCIZWUlOiOO+5QRkaGJMnn80mSUlJSwmpTUlKMaz6fT1FRUYqPj79oTXJycpfXTE5ODqsxv058fLyioqKMmkvh6RIAAGy0ePFilZSUhJ1zOp2X/L6HHnpIf/rTn1RbW9vlmsO01yMUCnU5Z2auOV/95dRcDJ0MAABMrOxkOJ1OxcXFhR2XChkLFy7UCy+8oJdfflnXX3+9cd7tdktSl05CU1OT0XVwu90KBoPy+/0XrTl+/HiX1z1x4kRYjfl1/H6/Ojo6unQ4LoSQAQBAHxEKhfTQQw/pV7/6lX7/+99r6NChYdeHDh0qt9utLVu2GOeCwaC2bt2q0aNHS5KysrLUv3//sJrGxkY1NDQYNTk5OWpubtauXbuMmp07d6q5uTmspqGhQY2NjUZNTU2NnE6nsrKyuvV+GJcAANBHPPjgg3r++ef161//WrGxsUYnweVyKTo6Wg6HQ8XFxSovL1daWprS0tJUXl6uAQMGqKCgwKidM2eOSktLlZiYqISEBJWVlSkzM1MTJkyQJA0bNkyTJ0/W3LlztWrVKknSvHnzlJeXp/T0dElSbm6uhg8fLq/Xq2XLlunUqVMqKyvT3Llzu/VkiUTIAACgi976xM8nn3xSkjRu3Liw808//bRmz54tSVq0aJHa29tVWFgov9+v7Oxs1dTUKDY21qhfuXKlIiMjNWPGDLW3t2v8+PFas2aNIiIijJq1a9eqqKjIeAolPz9flZWVxvWIiAht3LhRhYWFGjNmjKKjo1VQUKDly5d3+/3wORkAgCuO3Z+T8eWnrfuZtP7r9q61L2NPBgAAsAXjEgAAzPgFaZYgZAAAYELGsAYhAwAAE37VuzXYkwEAAGxBJwMAABM6GdagkwEAAGxByAAAALZgXAIAgAnjEmsQMgAAMCFjWINxCQAAsAWdDAAATBiXWIOQAQCACSHDGoxLAACALQgZAADAFoxLAAAwYVxiDUIGAAAmZAxrMC4BAAC2oJMBAIAJ4xJrEDIAADAhY1iDcQkAALAFnQwAAMxoZViCkAEAgAl7MqzBuAQAANiCTgYAACY0MqxByLgKvbqhTm/uflcnj51WZFSkUj/j1oT7RynJE2/UhEIhbV23W3W/e0MftgV03S0p+tLX71JyakIvrhywz7nOc/rDL3dr3x//rNbTH2hgfIw+f1e67vrybXL0++hHzh9+uUsN2w+q5f1WRURGaPDQQbpnZrauvyWll1cPqzEusQYh4yr01wPHdHtupjw3JevcuXP6/S926rmK36hw2f2Kuqa/JOmPv3lN2ze9rukL7lHi4Gv1yvo6PVv+gh5aUSBndFQvvwPAerUv7NWel/Zr+rfuUXJqgo69e0K//snv5RwQpVFTRkiSEgdfqy/NvlPxyXHqCHZqx+bX9Vz5b7TwiQcUExfdy+8A6HvYk3EV+vvF0/T5sZ9VcmqC3Dck6d4F96j5ZKsaD52Q9FEXY+fmP+nO6Vka9sWblZyaqOnfGq+O4Fnt++Pbvbx6wB5H3z6u9Ntu1GdG3qhrB8VpePbNuvnWVDW+e8KoyRzzGd2Umar4FJeSUxM06e/HKNAe1PHD7/fiymEHh4XH1YyQAQU+CEqSogc6JUmnm1rUevoD3ZyZatRE9o/QjcM8OvpnX6+sEbDbkPTBOtTwnt5vPC1J8v31pA6/2ahbPn/Dees7z3aq7vf75RwQJfeQxE9wpfgkOBzWHVczxiVXuVAopBef/aOGpA9WcupH/6Fsbf5AkjTQNSCsNsY1QM0nz3ziawQ+CWPyv6APPwiosvR59evXT+fOndM9M7KVOSYtrO7Pe/+iX/6oRh3Bs4q9Nkbeb0/TAEYlnzpXeTawjOWdjCNHjugb3/jGRWsCgYBaWlrCjo7gWauXgm7Y9PSrOn74fX1l4cSuF01/y0Kh0CezKKAX7N9+UPtq/6yvPDRR88q/qunfGq/tG+tVv/XNsLobh1+nBT+YqTnfvU83j0jVL39Yo7b/CeYAwlkeMk6dOqVnnnnmojUVFRVyuVxhxwtPb7F6KbiETU+/qj/XHdKsf71XcYkDjfMfdzBaT4f/h/ODlvYu3Q3g02LL2m0ac+9IZYxOU8qQRI24M12jpoxQ7Qt7w+qirumvBLdL16e5de/8e9Qvop/2vnygl1YNuzAusUaPxyUvvPDCRa+/++67l7zH4sWLVVJSEnZu/Rure7oUXKZQKKTNa17Vm7s/ChjxyXFh169NjtPAawfo3X1HNXjoIEkfzZ//cuCYJtyf0xtLBmzXETwrh+kngqOfQ6FzF+/ghUIhdZ7ttHNp6A1XeTiwSo9DxvTp0+VwOC7aOjf/RTVzOp1yOp1h5/pHsT3kk7LpP1/Rvm1v62ulU+SMjjI6Fs4BUeofFSmHw6HsKbfq1V/XKWGwS4lul17dsFf9oyK7zKeBT4vPjLxRr26okytxoJJTE9T4l5Pasel1fX7cMElS8MMOvbqhTulZN2rgtTFqb/1Qu7c0qOVUm4Zn39LLqwf6ph7/ZB88eLD+4z/+Q9OnTz/v9fr6emVlZf2t64KN9ry0X5L0zP/367Dz9y64R58f+1lJ0phpX9DZ4Flt+s9X1N4W0PU3p8j77Wl8RgY+tabMvlMv//+7tOnpV9TW3K7Y+Bhljf+cxn7lNklSv34OnTzm1+uvvKUPzrQreuA1uu7mZH39O9P5kLpPIRoZ1uhxyMjKytLevXsvGDIu1eVA7/vOfxVessbhcGjc331R4/7ui5/AioDe54yO0uRZd2jyrDvOez0yKlIzS6Z8wqtCb7na91JYpcch45/+6Z/U1tZ2weu33HKLXn755b9pUQAA4MrX45Bx5513XvR6TEyMxo4de9kLAgCgt9HIsAa7LQEAMGFcYg0+VhwAANiCTgYAACY0MqxByAAAwIRxiTUIGQAAmBAyrMGeDAAAYAs6GQAAmNDIsAYhAwAAE8Yl1mBcAgAAbEEnAwAAExoZ1iBkAABgwrjEGoxLAACALehkAABgQiPDGoQMAABMGJdYg3EJAACwBZ0MAABMaGRYg04GAAAmDod1R0+88sormjZtmjwejxwOhzZs2BB2PRQKacmSJfJ4PIqOjta4ceO0f//+sJpAIKCFCxcqKSlJMTExys/P19GjR8Nq/H6/vF6vXC6XXC6XvF6vTp8+HVZz+PBhTZs2TTExMUpKSlJRUZGCwWCP3g8hAwAAE4eFR0+0tbVpxIgRqqysPO/1pUuXasWKFaqsrNTu3bvldrs1ceJEnTlzxqgpLi7W+vXrVVVVpdraWrW2tiovL0+dnZ1GTUFBgerr61VdXa3q6mrV19fL6/Ua1zs7OzV16lS1tbWptrZWVVVVWrdunUpLS3v0fhyhUCjUwz8DWzy/94e9vQQAwBWiYOTDtt6/dKN1P5Men3p5a3U4HFq/fr2mT58u6aMuhsfjUXFxsR555BFJH3UtUlJS9Nhjj2n+/Plqbm7WoEGD9Oyzz2rmzJmSpGPHjik1NVWbNm3SpEmTdODAAQ0fPlw7duxQdna2JGnHjh3KycnRm2++qfT0dG3evFl5eXk6cuSIPB6PJKmqqkqzZ89WU1OT4uLiuvUe6GQAAGBi5bgkEAiopaUl7AgEAj1e06FDh+Tz+ZSbm2ucczqdGjt2rLZt2yZJqqurU0dHR1iNx+NRRkaGUbN9+3a5XC4jYEjSqFGj5HK5wmoyMjKMgCFJkyZNUiAQUF1dXbfXTMgAAMDEypBRUVFh7H34+KioqOjxmnw+nyQpJSUl7HxKSopxzefzKSoqSvHx8RetSU5O7nL/5OTksBrz68THxysqKsqo6Q6eLgEAwEaLFy9WSUlJ2Dmn03nZ93OYdpOGQqEu58zMNeerv5yaS6GTAQCAiZUbP51Op+Li4sKOywkZbrdbkrp0Epqamoyug9vtVjAYlN/vv2jN8ePHu9z/xIkTYTXm1/H7/ero6OjS4bgYQgYAACYOh8OywypDhw6V2+3Wli1bjHPBYFBbt27V6NGjJUlZWVnq379/WE1jY6MaGhqMmpycHDU3N2vXrl1Gzc6dO9Xc3BxW09DQoMbGRqOmpqZGTqdTWVlZ3V4z4xIAAPqI1tZWHTx40Pj60KFDqq+vV0JCgoYMGaLi4mKVl5crLS1NaWlpKi8v14ABA1RQUCBJcrlcmjNnjkpLS5WYmKiEhASVlZUpMzNTEyZMkCQNGzZMkydP1ty5c7Vq1SpJ0rx585SXl6f09HRJUm5uroYPHy6v16tly5bp1KlTKisr09y5c7v9ZIlEyAAAoIve+sTPPXv26O677za+/ngvx6xZs7RmzRotWrRI7e3tKiwslN/vV3Z2tmpqahQbG2t8z8qVKxUZGakZM2aovb1d48eP15o1axQREWHUrF27VkVFRcZTKPn5+WGfzREREaGNGzeqsLBQY8aMUXR0tAoKCrR8+fIevR8+JwMAcMWx+3MyFr/4I8vuVTGpyLJ7XWnYkwEAAGzBuAQAABN+QZo1CBkAAJj0I2VYgpABAIAJGcMa7MkAAAC2oJMBAICJhZ+hdVUjZAAAYELGsAbjEgAAYAs6GQAAmDAusQYhAwAAEzKGNRiXAAAAW9DJAADAhHGJNQgZAACYkDGswbgEAADYgk4GAAAm/O4SaxAyAAAwIWNYg5ABAIAJGz+twZ4MAABgCzoZAACY0MiwBiEDAAATxiXWYFwCAABsQScDAAATGhnWIGQAAGDCuMQajEsAAIAt6GQAAGBCJ8MahAwAAExo81uDP0cAAGALOhkAAJgwLrEGIQMAABMyhjUIGQAAmNDJsAZ7MgAAgC3oZAAAYEIjwxqEDAAATBiXWINxCQAAsAWdDAAATGhkWIOQAQCACeMSazAuAQAAtqCTAQCACY0MaxAyAAAwYVxiDcYlAADAFnQyAAAw4f/ArUHIAADAhHGJNQgZAACYkDGsQUcIAADYgk4GAAAmjEusQcgAAMCEjGENxiUAAMAWdDIAADBhXGINQgYAACaEDGswLgEAALagkwEAgAmNDGsQMgAAMGFcYg3GJQAAwBZ0MgAAMOH/wK1ByAAAwIRxiTUIGQAAmDgU6u0lfCrQEQIAALagkwEAgAnjEms4QqEQPSEYAoGAKioqtHjxYjmdzt5eDtAn8PcCuDyEDIRpaWmRy+VSc3Oz4uLiens5QJ/A3wvg8rAnAwAA2IKQAQAAbEHIAAAAtiBkIIzT6dR3vvMdNrcB/w/+XgCXh42fAADAFnQyAACALQgZAADAFoQMAABgC0IGAACwBSEDhh//+McaOnSorrnmGmVlZenVV1/t7SUBveqVV17RtGnT5PF45HA4tGHDht5eEnBFIWRAkvSLX/xCxcXFevTRR/Xaa6/pzjvv1JQpU3T48OHeXhrQa9ra2jRixAhVVlb29lKAKxKPsEKSlJ2drZEjR+rJJ580zg0bNkzTp09XRUVFL64M6BscDofWr1+v6dOn9/ZSgCsGnQwoGAyqrq5Oubm5Yedzc3O1bdu2XloVAOBKR8iATp48qc7OTqWkpISdT0lJkc/n66VVAQCudIQMGBwOR9jXoVCoyzkAALqLkAElJSUpIiKiS9eiqampS3cDAIDuImRAUVFRysrK0pYtW8LOb9myRaNHj+6lVQEArnSRvb0A9A0lJSXyer267bbblJOTo5/+9Kc6fPiwFixY0NtLA3pNa2urDh48aHx96NAh1dfXKyEhQUOGDOnFlQFXBh5hheHHP/6xli5dqsbGRmVkZGjlypW66667entZQK/5wx/+oLvvvrvL+VmzZmnNmjWf/IKAKwwhAwAA2II9GQAAwBaEDAAAYAtCBgAAsAUhAwAA2IKQAQAAbEHIAAAAtiBkAAAAWxAyAACALQgZAADAFoQMAABgC0IGAACwBSEDAADY4v8CEHd7NBpPmTIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, cmap='crest',annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
