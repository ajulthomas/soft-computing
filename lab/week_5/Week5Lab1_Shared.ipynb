{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4e8f0Jx4yor"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "amZWEJJs1q1y"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute  import KNNImputer\n",
        "\n",
        "\n",
        "#This function is used if one or more of your column has categorical data\n",
        "def convert_categorical_to_numeric(df, label_encoders, mode):\n",
        "    \"\"\"\n",
        "    Parameters: df the input DataFrame with some categorical columns, list of label encoders, mode ('train' or 'test').\n",
        "    Returns: df_converted with all columns converted to numeric types.\n",
        "    \"\"\"\n",
        "    # Create a copy of df to avoid modifying the original one\n",
        "    df_converted = df.copy()\n",
        "    # for each column in df\n",
        "    for column in range(len(df_converted.columns)):\n",
        "        # Initialise LabelEncoder that performs integer encoding as described in the lecture\n",
        "        if mode == 'develop':\n",
        "          label_encoders.append(LabelEncoder())\n",
        "        # Check if the column is of type object (string/categorical)\n",
        "        if df_converted.iloc[:,column].dtype == 'object':\n",
        "          # if so, conert using integer encoding\n",
        "          if mode == 'develop':\n",
        "            df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
        "          else:\n",
        "            df_converted.iloc[:,column] = label_encoders[column].transform(df_converted.iloc[:,column])\n",
        "    return df_converted, label_encoders\n",
        "\n",
        "\n",
        "#This function is used for converting model output into categorical form.\n",
        "#Used for classification tasks only\n",
        "def convert_numeric_to_categorical(y, label_encoders):\n",
        "    df_converted = pd.DataFrame(y)\n",
        "\n",
        "    # for each column in df\n",
        "    for column in range(len(df_converted.columns)):\n",
        "        # Check if the column is numerical\n",
        "        if pd.api.types.is_numeric_dtype(df_converted.iloc[:,column]):\n",
        "          df_converted.iloc[:,column] = label_encoders[column].inverse_transform(df_converted.iloc[:,column].astype(int))\n",
        "\n",
        "    return df_converted\n",
        "\n",
        "\n",
        "\n",
        "def replace_missing_values_with_neighours_data(data, n_neighbors=1):\n",
        "    # in this specific dataset, missing values are indicated by '?'\n",
        "    # so we will need to first replace all '?' values with NaN\n",
        "    df_replaced = data.replace('?', np.nan)\n",
        "\n",
        "    # Initialize KNNImputer with the specified number of neighbors\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "    df_numeric = df_replaced.apply(pd.to_numeric, errors='coerce')\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
        "    return df_imputed\n",
        "\n",
        "\n",
        "\n",
        "# This function can be used for data augmentation\n",
        "# Below is just one example of data augmentation, but you can use any other suitable data augmentation technique\n",
        "# If you don't want to use any data augmentation for your task, then in the first line of the function write return X,y\n",
        "def augment_data(X,y, noise_factor=0.05):\n",
        "  \"\"\"\n",
        "  Augments the data by adding noise to the 'capital.gain' and 'capital.loss' columns.\n",
        "\n",
        "  Parameters:  X,y: data before augmentation\n",
        "              noise_factor: The factor to multiply the standard deviation of the columns by to generate noise.\n",
        "\n",
        "  Returns:The augmented data\n",
        "  \"\"\"\n",
        "  # Make a copy of the data to avoid modifying the original\n",
        "  x_noisy = X.copy()\n",
        "\n",
        "  # Add noise to 'capital.gain' which is column # 10\n",
        "  noise_gain = np.random.normal(0, noise_factor * X[:,10].std(), size=len(X[:,10]))\n",
        "  x_noisy[:,10] += noise_gain\n",
        "\n",
        "  # Add noise to 'capital.loss' which is column # 11\n",
        "  noise_loss = np.random.normal(0, noise_factor * X[:,11].std(), size=len(X[:,11]))\n",
        "  x_noisy[:,11] += noise_loss\n",
        "\n",
        "  #use both original & newly created datapoints for training\n",
        "  x_augmented = np.vstack([x_noisy, X])\n",
        "  y_augmented = np.concatenate([y, y])\n",
        "  return x_augmented, y_augmented\n",
        "\n",
        "\n",
        "\n",
        "# This function can be used for feature engineering\n",
        "# Below is just one example of feature engineering where we kept the original raw data and added an extra column. You can use any other features as deemed appropriate for your problem\n",
        "def feature_engineering(x):\n",
        "  # todo: add new features as needed\n",
        "  x_copy = x.copy()\n",
        "  #example, here we add a new feature: capital.net calculated as the capital.gain-capital.loss\n",
        "  capital_net = x_copy[:,10] - x_copy[:,11]\n",
        "\n",
        "  x_copy = np.hstack((x_copy, capital_net.reshape(-1,1)))\n",
        "  return x_copy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWugeObf48Rz"
      },
      "source": [
        "# DevelopAndEvaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnjIZsB31y1d",
        "outputId": "aff5e630-646e-43a8-a125-7d566ea201ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading data complete\n",
            "categorical to numerical conversion complete\n",
            "shape of data before categorical concersion (19536, 15)\n",
            "shape of data after categorical concersion (19536, 15)\n",
            "missing data replacement complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "C:\\Users\\u3253992\\AppData\\Local\\Temp\\ipykernel_13348\\1812765236.py:25: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_converted.iloc[:,column] = label_encoders[column].fit_transform(df_converted.iloc[:,column])\n",
            "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training score: 0.84\n",
            "Testing score: 0.8392765739634874\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# if mode is 'develop', it means that we will use the available data for training our model\n",
        "#if mode is 'evaluate', it means that we will load the model that was trained before (as well as any operators for data pre-processing) to make prediction on novel data\n",
        "mode = 'develop'\n",
        "\n",
        "#How many outputs (i.e., columns) your model should predict\n",
        "num_outputs  = 1\n",
        "\n",
        "if mode == 'develop':\n",
        "  #read data from file\n",
        "  all_data = pd.read_csv('./data/adult1.csv')\n",
        "  print(\"reading data complete\")\n",
        "\n",
        "  #convert categorical data to numeric (similar to what we did in Lecture 4 - page 10)\n",
        "  label_encoders = []\n",
        "  all_data_numeric,label_encoders = convert_categorical_to_numeric(all_data, label_encoders, mode)\n",
        "  print(\"categorical to numerical conversion complete\")\n",
        "  print(\"shape of data before categorical concersion\", all_data.shape)\n",
        "  print(\"shape of data after categorical concersion\", all_data_numeric.shape)\n",
        "\n",
        "\n",
        "  #replace missing data with data from the most similar raw (similar to what we did in Lecture 4 - page 6)\n",
        "  all_data_numeric_no_missing = replace_missing_values_with_neighours_data(all_data_numeric)\n",
        "  print(\"missing data replacement complete\")\n",
        "\n",
        "\n",
        "  X = all_data_numeric_no_missing.iloc[:, :-num_outputs] # inputs\n",
        "  y = all_data_numeric_no_missing.iloc[:, -num_outputs:] # outputs\n",
        "  # split into train & test sets\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
        "\n",
        "  # Standardise the traing data (similar to what we did in Lecture 4 - pages 15-16)\n",
        "  scaler = StandardScaler()\n",
        "  x_train_scaled = scaler.fit_transform(x_train)\n",
        "\n",
        "\n",
        "  #augment data (similar to what we did in Lecture 4 - pages 39)\n",
        "  x_train_augmented, y_train_augmented = augment_data(x_train_scaled,y_train)\n",
        "\n",
        "  # Apply feature engineering\n",
        "  x_train_feature_engineered = feature_engineering(x_train_augmented)\n",
        "\n",
        "  # Apply PCA to reduce the dimensionality of the data (similar to what we did in Lecture 4 - pages 28-29)\n",
        "  pca = PCA(n_components=8)\n",
        "  pca.fit(x_train_feature_engineered)\n",
        "  x_train_pca = pca.transform(x_train_feature_engineered)\n",
        "\n",
        "  #Train the model\n",
        "  # we specify that 10% of the training data will be used for validation. Ealy stopping is applied (similar to what we did in Lecture 4, page 33)\n",
        "  # we also use alpha =0.1 to activate L2 regularisation\n",
        "  model = MLPClassifier(hidden_layer_sizes=(24, 48), activation= 'relu', early_stopping = True, validation_fraction =0.1,alpha =0.1 , verbose = False, learning_rate_init=0.01, batch_size=80 )\n",
        "  model.fit(x_train_pca, y_train_augmented)\n",
        "  train_score = model.score(x_train_pca, y_train_augmented)\n",
        "  print(\"Training score:\", train_score)\n",
        "\n",
        "  # test model\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  x_test_feature_engineered = feature_engineering(x_test_scaled)\n",
        "  x_test_pca = pca.transform(x_test_feature_engineered)\n",
        "  test_score = model.score(x_test_pca, y_test)\n",
        "  print(\"Testing score:\", test_score)\n",
        "\n",
        "  # now save the model, the PCA object, the scaler object & the encoders so that you can load them at a later point & use them for novel data\n",
        "  with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "  with open('pca.pkl', 'wb') as f:\n",
        "    pickle.dump(pca, f)\n",
        "\n",
        "  with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "  with open('label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "elif mode == 'evaluate':\n",
        "  # read the pre-trained model, the PCA object & the scaler object & encoders\n",
        "  model = pickle.load(open('model.pkl', 'rb'))\n",
        "  pca = pickle.load(open('pca.pkl', 'rb'))\n",
        "  scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
        "  label_encoders= pickle.load(open('label_encoders.pkl', 'rb'))\n",
        "\n",
        "  # Read novel data. This data usually has no output y. You'll need to use your model to predict y\n",
        "  novel_data= pd.read_csv('./data/adult2.csv')\n",
        "\n",
        "  # you will need to do the exact same pre-preocessing for this novel data similar to what you did for your training data\n",
        "\n",
        "  #convert categorical data to numeric\n",
        "  novel_data_numeric, _ = convert_categorical_to_numeric(novel_data, label_encoders, mode)\n",
        "\n",
        "  #replace missing data with data from the most similar raw\n",
        "  novel_data_no_missing = replace_missing_values_with_neighours_data(novel_data_numeric)\n",
        "\n",
        "\n",
        "\n",
        "  X_novel = novel_data_no_missing    # novel data will not contain any output; so use all columns as X\n",
        "  X_novel = scaler.transform(X_novel)\n",
        "  # standardise\n",
        "  X_novel = feature_engineering(X_novel)\n",
        "  # get pca components\n",
        "  X_novel_pca = pca.transform(X_novel)\n",
        "  y_predict = model.predict(X_novel_pca)\n",
        "  print(\"prediction complete\")\n",
        "  #if output was originally categorical, then convert back to its original form\n",
        "  y_predict = convert_numeric_to_categorical(y_predict, label_encoders[-num_outputs:])\n",
        "  print(y_predict)\n",
        "  #save the prediction to a csv file\n",
        "  pd.DataFrame(y_predict).to_csv('prediction.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
