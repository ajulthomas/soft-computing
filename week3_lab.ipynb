{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "1. Load Data\n",
    "2. Divide into train and test set. eg, divide 20% for test set and 80% train\n",
    "                                    divide train into 20% for validation and 80% for train set\n",
    "3. fit the model: calc taining and vlidation accuracy\n",
    "\n",
    "4. Fine tune the model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loding Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "feature_names= california.feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data\n",
    "slipt the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_validate, x_test, y_train_validate, y_test = train_test_split(X,y, test_size=0.2, random_state = 33)\n",
    "x_train, x_val,y_train, y_val = train_test_split(x_train_validate, y_train_validate, test_size=0.2, random_state = 33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation\n",
    "standardise the range of all features, so they all have mean of zero and a standard deviation of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network/ MLP(Milti-layer perceptron)\n",
    "\n",
    "learn the relationship b/w the target and the features using neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41175467\n",
      "Iteration 2, loss = 0.42201647\n",
      "Iteration 3, loss = 0.33748503\n",
      "Iteration 4, loss = 0.29620758\n",
      "Iteration 5, loss = 0.26732617\n",
      "Iteration 6, loss = 0.24726687\n",
      "Iteration 7, loss = 0.23304252\n",
      "Iteration 8, loss = 0.22521502\n",
      "Iteration 9, loss = 0.21753250\n",
      "Iteration 10, loss = 0.21158092\n",
      "Iteration 11, loss = 0.20711490\n",
      "Iteration 12, loss = 0.20391714\n",
      "Iteration 13, loss = 0.20075189\n",
      "Iteration 14, loss = 0.19858682\n",
      "Iteration 15, loss = 0.19580087\n",
      "Iteration 16, loss = 0.19371067\n",
      "Iteration 17, loss = 0.19177492\n",
      "Iteration 18, loss = 0.18953445\n",
      "Iteration 19, loss = 0.18803462\n",
      "Iteration 20, loss = 0.18641568\n",
      "Iteration 21, loss = 0.18505540\n",
      "Iteration 22, loss = 0.18418291\n",
      "Iteration 23, loss = 0.18217849\n",
      "Iteration 24, loss = 0.18153413\n",
      "Iteration 25, loss = 0.18062711\n",
      "Iteration 26, loss = 0.17863024\n",
      "Iteration 27, loss = 0.17695705\n",
      "Iteration 28, loss = 0.17559920\n",
      "Iteration 29, loss = 0.17528520\n",
      "Iteration 30, loss = 0.17495347\n",
      "Iteration 31, loss = 0.17293586\n",
      "Iteration 32, loss = 0.17187598\n",
      "Iteration 33, loss = 0.17223707\n",
      "Iteration 34, loss = 0.17206680\n",
      "Iteration 35, loss = 0.16954844\n",
      "Iteration 36, loss = 0.16805480\n",
      "Iteration 37, loss = 0.16695773\n",
      "Iteration 38, loss = 0.16700148\n",
      "Iteration 39, loss = 0.16763806\n",
      "Iteration 40, loss = 0.16530232\n",
      "Iteration 41, loss = 0.16472730\n",
      "Iteration 42, loss = 0.16315760\n",
      "Iteration 43, loss = 0.16446016\n",
      "Iteration 44, loss = 0.16269493\n",
      "Iteration 45, loss = 0.16174683\n",
      "Iteration 46, loss = 0.16238999\n",
      "Iteration 47, loss = 0.16449403\n",
      "Iteration 48, loss = 0.16647800\n",
      "Iteration 49, loss = 0.16218236\n",
      "Iteration 50, loss = 0.16136729\n",
      "Iteration 51, loss = 0.16002715\n",
      "Iteration 52, loss = 0.15790331\n",
      "Iteration 53, loss = 0.15833827\n",
      "Iteration 54, loss = 0.15760494\n",
      "Iteration 55, loss = 0.15717548\n",
      "Iteration 56, loss = 0.15577290\n",
      "Iteration 57, loss = 0.15464556\n",
      "Iteration 58, loss = 0.15501483\n",
      "Iteration 59, loss = 0.15497462\n",
      "Iteration 60, loss = 0.15445430\n",
      "Iteration 61, loss = 0.15431560\n",
      "Iteration 62, loss = 0.15416318\n",
      "Iteration 63, loss = 0.16296058\n",
      "Iteration 64, loss = 0.15458715\n",
      "Iteration 65, loss = 0.15445083\n",
      "Iteration 66, loss = 0.15302428\n",
      "Iteration 67, loss = 0.15403935\n",
      "Iteration 68, loss = 0.15126340\n",
      "Iteration 69, loss = 0.15140370\n",
      "Iteration 70, loss = 0.15055664\n",
      "Iteration 71, loss = 0.15075270\n",
      "Iteration 72, loss = 0.15202292\n",
      "Iteration 73, loss = 0.15122918\n",
      "Iteration 74, loss = 0.14977036\n",
      "Iteration 75, loss = 0.14962965\n",
      "Iteration 76, loss = 0.15205954\n",
      "Iteration 77, loss = 0.14992959\n",
      "Iteration 78, loss = 0.15027730\n",
      "Iteration 79, loss = 0.14950521\n",
      "Iteration 80, loss = 0.14872512\n",
      "Iteration 81, loss = 0.15046674\n",
      "Iteration 82, loss = 0.14896766\n",
      "Iteration 83, loss = 0.14790558\n",
      "Iteration 84, loss = 0.14783787\n",
      "Iteration 85, loss = 0.14818385\n",
      "Iteration 86, loss = 0.14675468\n",
      "Iteration 87, loss = 0.14763809\n",
      "Iteration 88, loss = 0.14735481\n",
      "Iteration 89, loss = 0.14983191\n",
      "Iteration 90, loss = 0.15013821\n",
      "Iteration 91, loss = 0.14727577\n",
      "Iteration 92, loss = 0.14702773\n",
      "Iteration 93, loss = 0.14800712\n",
      "Iteration 94, loss = 0.14953716\n",
      "Iteration 95, loss = 0.14654465\n",
      "Iteration 96, loss = 0.14636073\n",
      "Iteration 97, loss = 0.14546973\n",
      "Iteration 98, loss = 0.14578260\n",
      "Iteration 99, loss = 0.14537871\n",
      "Iteration 100, loss = 0.14560059\n",
      "Iteration 101, loss = 0.14477981\n",
      "Iteration 102, loss = 0.14453647\n",
      "Iteration 103, loss = 0.14545991\n",
      "Iteration 104, loss = 0.15109601\n",
      "Iteration 105, loss = 0.14548770\n",
      "Iteration 106, loss = 0.14944975\n",
      "Iteration 107, loss = 0.14564813\n",
      "Iteration 108, loss = 0.14820915\n",
      "Iteration 109, loss = 0.14463947\n",
      "Iteration 110, loss = 0.14688001\n",
      "Iteration 111, loss = 0.14840354\n",
      "Iteration 112, loss = 0.14372018\n",
      "Iteration 113, loss = 0.14430915\n",
      "Iteration 114, loss = 0.14364737\n",
      "Iteration 115, loss = 0.14332748\n",
      "Iteration 116, loss = 0.14380092\n",
      "Iteration 117, loss = 0.14332492\n",
      "Iteration 118, loss = 0.14552284\n",
      "Iteration 119, loss = 0.14384763\n",
      "Iteration 120, loss = 0.14324233\n",
      "Iteration 121, loss = 0.14335652\n",
      "Iteration 122, loss = 0.14364901\n",
      "Iteration 123, loss = 0.14224633\n",
      "Iteration 124, loss = 0.14480942\n",
      "Iteration 125, loss = 0.14340479\n",
      "Iteration 126, loss = 0.14414174\n",
      "Iteration 127, loss = 0.14212651\n",
      "Iteration 128, loss = 0.14220280\n",
      "Iteration 129, loss = 0.14304816\n",
      "Iteration 130, loss = 0.14389137\n",
      "Iteration 131, loss = 0.14785317\n",
      "Iteration 132, loss = 0.14356423\n",
      "Iteration 133, loss = 0.14142968\n",
      "Iteration 134, loss = 0.14139536\n",
      "Iteration 135, loss = 0.14299938\n",
      "Iteration 136, loss = 0.14157954\n",
      "Iteration 137, loss = 0.14297505\n",
      "Iteration 138, loss = 0.14228218\n",
      "Iteration 139, loss = 0.14165646\n",
      "Iteration 140, loss = 0.14256562\n",
      "Iteration 141, loss = 0.14079123\n",
      "Iteration 142, loss = 0.14185328\n",
      "Iteration 143, loss = 0.14168873\n",
      "Iteration 144, loss = 0.14258331\n",
      "Iteration 145, loss = 0.14203279\n",
      "Iteration 146, loss = 0.14054717\n",
      "Iteration 147, loss = 0.14105089\n",
      "Iteration 148, loss = 0.14070897\n",
      "Iteration 149, loss = 0.14934166\n",
      "Iteration 150, loss = 0.14920888\n",
      "Iteration 151, loss = 0.14375325\n",
      "Iteration 152, loss = 0.14034333\n",
      "Iteration 153, loss = 0.14155924\n",
      "Iteration 154, loss = 0.14633477\n",
      "Iteration 155, loss = 0.15473911\n",
      "Iteration 156, loss = 0.14650526\n",
      "Iteration 157, loss = 0.13997808\n",
      "Iteration 158, loss = 0.14162997\n",
      "Iteration 159, loss = 0.14076315\n",
      "Iteration 160, loss = 0.14055477\n",
      "Iteration 161, loss = 0.14005594\n",
      "Iteration 162, loss = 0.14041462\n",
      "Iteration 163, loss = 0.13969494\n",
      "Iteration 164, loss = 0.13931751\n",
      "Iteration 165, loss = 0.13913586\n",
      "Iteration 166, loss = 0.13906679\n",
      "Iteration 167, loss = 0.13933814\n",
      "Iteration 168, loss = 0.13953617\n",
      "Iteration 169, loss = 0.14023247\n",
      "Iteration 170, loss = 0.14033749\n",
      "Iteration 171, loss = 0.14163415\n",
      "Iteration 172, loss = 0.13866122\n",
      "Iteration 173, loss = 0.13943223\n",
      "Iteration 174, loss = 0.13865092\n",
      "Iteration 175, loss = 0.13821846\n",
      "Iteration 176, loss = 0.13999791\n",
      "Iteration 177, loss = 0.13980276\n",
      "Iteration 178, loss = 0.15881279\n",
      "Iteration 179, loss = 0.13992506\n",
      "Iteration 180, loss = 0.13910771\n",
      "Iteration 181, loss = 0.13992806\n",
      "Iteration 182, loss = 0.13834197\n",
      "Iteration 183, loss = 0.13838961\n",
      "Iteration 184, loss = 0.13833808\n",
      "Iteration 185, loss = 0.13802414\n",
      "Iteration 186, loss = 0.13902308\n",
      "Iteration 187, loss = 0.13961404\n",
      "Iteration 188, loss = 0.14039870\n",
      "Iteration 189, loss = 0.13924534\n",
      "Iteration 190, loss = 0.13862115\n",
      "Iteration 191, loss = 0.13823580\n",
      "Iteration 192, loss = 0.13869333\n",
      "Iteration 193, loss = 0.13854373\n",
      "Iteration 194, loss = 0.13843619\n",
      "Iteration 195, loss = 0.13781818\n",
      "Iteration 196, loss = 0.13735849\n",
      "Iteration 197, loss = 0.13758537\n",
      "Iteration 198, loss = 0.13713433\n",
      "Iteration 199, loss = 0.14064110\n",
      "Iteration 200, loss = 0.13804779\n",
      "Training score of MLP: 0.7933058287701625\n",
      "Validation score of MLP: 0.775052383752764\n",
      "accuracy on unseen  of MLP: 0.7960848313591176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tshering Gyeltshen\\miniconda3\\envs\\mds\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(verbose= True).fit(x_train_scaled, y_train)\n",
    "print(\"Training score of MLP:\", mlp_regressor.score(x_train_scaled,y_train))\n",
    "\n",
    "# also standardise test data\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "print(\"Validation score of MLP:\", mlp_regressor.score(x_val_scaled, y_val))\n",
    "\n",
    "# also standardise test data\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "print(\"accuracy on unseen  of MLP:\", mlp_regressor.score(x_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_regressor = MLPRegressor(verbose=True, hidden_layer_sizes=(50,), activation='relu',\n",
    "                             solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
